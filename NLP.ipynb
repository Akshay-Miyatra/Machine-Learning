{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0816e1fb-94c9-4b50-8da7-131b9dc6362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\atmiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unit 4 (nlp) nathural lanaguage processing\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48ebedd-5dff-44dc-9d6d-bb42a66c29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i love nlp.it's fun,isn't it?\"]\n"
     ]
    }
   ],
   "source": [
    "#Applying the sentence tokenization on the text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"i love nlp.it's fun,isn't it?\"\n",
    "tokenize_sentance = sent_tokenize(text)\n",
    "print(tokenize_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fdf9fa-158b-4ee8-b650-7282c1206892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word punctuation tokenizer ['I', 'love', 'NLP', '.', 'It', \"'\", 's', 'fun', '.', 'isn', \"'\", 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "#punctuation word tokenize on the text\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"I love NLP.It's fun.isn't it?\"\n",
    "tokenised_wordpunch = wordpunct_tokenize(text)\n",
    "print(\"word punctuation tokenizer\",tokenised_wordpunch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8f5089-1021-4de8-8e81-35b4cbb9766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter stemmer\n",
      "student --> student\n",
      "playing --> play\n",
      "played --> play\n",
      "studies --> studi\n",
      "happiness --> happi\n",
      "snowball stemmer\n",
      "student --> student\n",
      "playing --> play\n",
      "played --> play\n",
      "studies --> studi\n",
      "happiness --> happi\n",
      "Lancaster stemmer\n",
      "student --> stud\n",
      "playing --> play\n",
      "played --> play\n",
      "studies --> study\n",
      "happiness --> happy\n"
     ]
    }
   ],
   "source": [
    "#steming all 3\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
    "\n",
    "porter =PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lan = LancasterStemmer()\n",
    "\n",
    "words = [\"student\",\"playing\",\"played\",\"studies\",\"happiness\"]\n",
    "\n",
    "print(\"porter stemmer\")\n",
    "for w in words:\n",
    "    print(w,\"-->\",porter.stem(w))\n",
    "    \n",
    "print(\"snowball stemmer\")\n",
    "for w in words:\n",
    "    print(w,\"-->\",snowball.stem(w))\n",
    "    \n",
    "print(\"Lancaster stemmer\")\n",
    "for w in words:\n",
    "    print(w,\"-->\",lan.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ec3378-df04-4864-aeb7-53475e32fe26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be57206b909d44c1a768cbe4b0730a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 13:12:18 INFO: Downloaded file to C:\\Users\\atmiya\\stanza_resources\\resources.json\n",
      "2025-09-03 13:12:18 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-09-03 13:12:19 INFO: File exists: C:\\Users\\atmiya\\stanza_resources\\en\\default.zip\n",
      "2025-09-03 13:12:23 INFO: Finished downloading models and saved to C:\\Users\\atmiya\\stanza_resources\n",
      "2025-09-03 13:12:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82364ff49fcc4255904c95543d6e29de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 13:12:23 INFO: Downloaded file to C:\\Users\\atmiya\\stanza_resources\\resources.json\n",
      "2025-09-03 13:12:24 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-09-03 13:12:24 INFO: Using device: cpu\n",
      "2025-09-03 13:12:24 INFO: Loading: tokenize\n",
      "2025-09-03 13:12:24 INFO: Loading: mwt\n",
      "2025-09-03 13:12:24 INFO: Loading: pos\n",
      "2025-09-03 13:12:26 INFO: Loading: lemma\n",
      "2025-09-03 13:12:26 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> the\n",
      "children --> child\n",
      "are --> be\n",
      "learning --> learn\n",
      "with --> with\n",
      "their --> their\n",
      "toys --> toy\n",
      "and --> and\n",
      "games --> game\n",
      "are --> be\n",
      "ongoing --> ongoing\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "#pip install stanza\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "text = \"The children are learning with their toys and games are ongoing\"\n",
    "doc = nlp(text)\n",
    "for s in doc.sentences:\n",
    "    for word in s.words:\n",
    "        print(word.text,\"-->\",word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2eb4a56-43da-4468-bbca-a02284569c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\atmiya\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment analysis\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11486bf-0d80-4824-ada8-edcb99a55413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like this product! It's amazinjg --> {'neg': 0.0, 'neu': 0.589, 'pos': 0.411, 'compound': 0.4199}\n",
      "this is the worst experience ever --> {'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.6249}\n",
      "The product is okay,not great but not bad either. --> {'neg': 0.0, 'neu': 0.525, 'pos': 0.475, 'compound': 0.745}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentences = {\n",
    "    \"i like this product! It's amazinjg\",\n",
    "    \"this is the worst experience ever\",\n",
    "    \"The product is okay,not great but not bad either.\"\n",
    "}\n",
    "for text in sentences:\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    print(text,'-->',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6303c4-48c2-4673-824f-6bcc7495d999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
